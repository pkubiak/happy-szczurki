{
    "module": "happy_szczurki.models.cnn.ConvNet",
    "optimizer__weight_decay": 0.0001,

    "module__input_shape": [1, 127, 127],
    "module__classes": 11,
    "module__conv_layers": [
        {
            // NOTE: https://stackoverflow.com/questions/45463778/instance-normalisation-vs-batch-normalisation
            // ~ IN provide visual and appearance in-variance and BN accelerate training and preserve discriminative feature. 
            "instance_norm": true
        },
        {
            "conv": {"out_channels": 32, "kernel_size": 5, "stride": 1, "padding": 2},
            "activation": "relu",
            "batch_norm": true,
            "dropout": 0.0
        },
        {
            "conv": {"out_channels": 64, "kernel_size": 3, "stride": 2, "padding": 1},
            "activation": "relu",
            "batch_norm": true,
            "dropout": 0.0
        },
        {
            "conv": {"out_channels": 128, "kernel_size": 3, "stride": 2, "padding": 1},
            "activation": "relu",
            "batch_norm": true,
            "dropout": 0.0
        },
        // Global Convolution Layer
        { 
            "conv": {"out_channels": 128, "kernel_size": 32, "stride":1, "padding": 0, "groups": 128},
            "activation": "relu",
            "batch_norm": false,
            "dropout": 0.0
        }
        // {
        //     "conv": {"out_channels": 16, "kernel_size": 1, "stride": 1, "padding": 0},
        //     "activation": "relu",
        //     "batch_norm": true,
        //     "dropout": 0.0
        // }
    ], 
    "module__linear_layers": [
        // {"out_features": 256, "activation": "relu", "dropout": 0.5},
        {"out_features": 64, "activation": "relu", "dropout": 0.5}
    ]
}
