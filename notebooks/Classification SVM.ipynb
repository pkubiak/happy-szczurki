{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO ##\n",
    "- dodać lime\n",
    "- wydzielić sensowne fragmenty kodu do plików pythonowych\n",
    "- odpalić kod na innych datasetach\n",
    "- odpalić więcej random-searchów\n",
    "- dodać readme\n",
    "- odpalić na obciętym datasecie\n",
    "- wrzucić rezultaty do README.md\n",
    "- https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Importing%20Notebooks.html\n",
    "\n",
    "---\n",
    "\n",
    "- na całym zbiorze danych\n",
    "- random forest + xgboost\n",
    "- CNN\n",
    "- grid search z i be normalizacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn import  metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace \n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        dataset = dict(np.load(self.path, allow_pickle=True))\n",
    "        \n",
    "        self.X = dataset.pop('X')\n",
    "        self.y = dataset.pop('y')\n",
    "        self.meta = dataset.pop('meta')[()]\n",
    "        \n",
    "        assert len(dataset) == 0\n",
    "        \n",
    "        self.y_binary = np.where(self.y == None, 0, 1)\n",
    "    \n",
    "    def normalize(self, mean=None, std=None):\n",
    "        # TODO: czy powinniśmy normalizować per współrzędna czy globalnie?\n",
    "        if mean is None:\n",
    "            mean = np.mean(self.X, axis=0)\n",
    "            \n",
    "        if std is None:\n",
    "            std = np.std(self.X, axis=0, ddof=1)\n",
    "            \n",
    "        # NOTE: inplace operators to prevent memory allocations\n",
    "        self.X -= mean\n",
    "        self.X /= std\n",
    "        \n",
    "        return mean, std\n",
    "    \n",
    "    def sample(self, n, *, balanced=False, with_idx=False, random_state=None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Choice `n` random samples from dataset.\n",
    "        \n",
    "        @param n: number of random samples to choose,\n",
    "        @param balanced: if True number of samples for each class will be aprox. equal,\n",
    "        @param with_idx: return data indexes of sampled records,\n",
    "        @param random_state: random state used to generate samples indices,\n",
    "        \"\"\"\n",
    "        assert len(self.y_binary.shape) == 1\n",
    "        \n",
    "        if balanced:\n",
    "            counts = Counter(self.y_binary)\n",
    "            class_count = len(counts)\n",
    "            \n",
    "            # NOTE: https://stackoverflow.com/questions/35215161/most-efficient-way-to-map-function-over-numpy-array/35216364\n",
    "            probs = np.array([1.0 / (class_count * counts[x]) for x in self.y_binary])\n",
    "        else:\n",
    "            probs = None\n",
    "            \n",
    "        idx = np.random.RandomState(random_state).choice(self.y.size, size=n, p=probs)\n",
    "        \n",
    "        if with_idx:\n",
    "            return idx, self.X[idx], self.y_binary[idx]\n",
    "        \n",
    "        return self.X[idx], self.y_binary[idx]\n",
    "    \n",
    "    def frame_to_time(self, frame: int) -> float:\n",
    "        \"\"\"Convert frame id to time in sec.\"\"\"\n",
    "        return librosa.core.frames_to_time(\n",
    "            frame,\n",
    "            sr=self.meta['sampling_rate'],\n",
    "            hop_length=self.meta['hop_length'],\n",
    "            n_fft=self.meta['n_fft']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ch1-2018-11-20_10-29-02_0000012.wav.trimed.npz Counter({0: 196372, 1: 38013})\n",
      "mean: 0.000000; std: 1.000209\n"
     ]
    }
   ],
   "source": [
    "train_data = test_data = None\n",
    "train_data = Dataset('ch1-2018-11-20_10-29-02_0000012.wav.trimed.npz')\n",
    "print(train_data.path, Counter(train_data.y_binary))\n",
    "\n",
    "mean, std = train_data.normalize()\n",
    "print(\"mean: %f; std: %f\" % (np.mean(train_data.X), np.std(train_data.X, ddof=1)))\n",
    "\n",
    "#################################################################\n",
    "# print()\n",
    "# test_data = Dataset('ch1-2018-11-20_10-26-36_0000010.wav.trimed.npz')\n",
    "# print(test_data.path, Counter(test_data.y_binary))\n",
    "\n",
    "# test_data.normalize(mean, std)\n",
    "# print(\"mean: %f; std: %f\" % (np.mean(test_data.X), np.std(test_data.X, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.sample(10, with_idx=True, random_state=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.8617641e-02,  3.6422275e-02,  6.0354851e-02, ...,\n",
       "         3.9282343e-01, -7.7947730e-01,  1.7884290e-01],\n",
       "       [ 9.4728256e-03,  1.6607676e-02, -4.8397966e-03, ...,\n",
       "        -3.6532238e-02,  2.1068742e-02,  5.7583737e-01],\n",
       "       [-9.2739612e-03, -1.0576712e-02, -1.9489793e-05, ...,\n",
       "         3.3974552e-01,  3.7376234e-01,  6.3606781e-01],\n",
       "       ...,\n",
       "       [-9.2718471e-03, -7.2237165e-03, -4.6709487e-03, ...,\n",
       "         4.3271102e-02, -7.5808579e-01, -5.9807819e-01],\n",
       "       [-2.4606441e-03,  5.1627546e-03, -3.7304866e-03, ...,\n",
       "         1.5693192e-01, -7.1247154e-01, -3.9950964e-01],\n",
       "       [ 5.5131242e-02,  9.0585597e-02,  1.7826210e-01, ...,\n",
       "         2.0846680e-01, -3.6042258e-01, -9.0065479e-02]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM classification #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(train_data.X.T)\n",
    "print(corr.shape)\n",
    "\n",
    "plt.figure(figsize=(14,12))\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data for training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = 5000\n",
    "\n",
    "X, y = train_data.sample(N_samples, balanced=True, random_state=43)\n",
    "\n",
    "print(X.shape)\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test baseline model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, n=10000, balanced=False):\n",
    "    idx, X, y_true = dataset.sample(n, balanced=balanced, with_idx=True)\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "    print('accuracy:', accuracy_score(y_true, y_pred))\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "    print('precision:', precision)\n",
    "    print('recall:', recall)\n",
    "    print('fscore:', fscore)\n",
    "    c = Counter(y_true)\n",
    "    print('support:', c)\n",
    "    \n",
    "    return idx, y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = SVC(kernel='rbf', gamma='auto') # 0.9021\n",
    "# model = SVC(kernel='poly', degree=3, gamma='auto') # 0.93335\n",
    "# model = SVC(kernel='poly', degree=2, gamma='auto') # 0.9414\n",
    "# model = SVC(kernel='linear', degree=1, gamma='auto') # 0.9402\n",
    "# model = SVC(kernel='sigmoid', gamma='auto')\n",
    "model = SVC(kernel='poly', degree=1, gamma='auto') # 0.9424\n",
    "\n",
    "c = Counter(y)\n",
    "print(c)\n",
    "print('Baselines:')\n",
    "print('weighted random accuracy:', sum(i*i for i in c.values()) / sum(c.values())**2)\n",
    "print('max random accuracy:', max(c.values()) / sum(c.values()))\n",
    "print()\n",
    "\n",
    "model.fit(X, y)\n",
    "print('Done')\n",
    "\n",
    "# Evaluation\n",
    "print('Trainset:')\n",
    "evaluate_model(model, train_data)\n",
    "\n",
    "print()\n",
    "print('TestSet:')\n",
    "print('unbalanced:')\n",
    "evaluate_model(model, test_data)\n",
    "print('balanced:')\n",
    "evaluate_model(model, test_data, balanced=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters optimization (with RandomizedSearch) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: brudne haxy\n",
    "import signal, time, sys, os\n",
    "import multiprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "class FitTimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def timeout(clf, timeout: float = 120.0):\n",
    "    \"\"\"Estimators' decorator for timouting fitting process after `timeout` seconds.\"\"\"\n",
    "    def fit(self, *args, _timeout=timeout, **kwargs):\n",
    "        pool = multiprocessing.pool.ThreadPool(1)\n",
    "        async_result = pool.apply_async(clf.fit, (self, *args), kwargs)\n",
    "        try:\n",
    "            return async_result.get(_timeout)\n",
    "        except multiprocessing.TimeoutError as e:\n",
    "            raise FitTimeoutError(f\"fit timeout after {timeout:.2f}s\") from e\n",
    "            \n",
    "    name = f\"{clf.__name__}_with_timeout\"\n",
    "    cls = type(name, (clf,), dict(fit=fit))   \n",
    "    \n",
    "    setattr(sys.modules['abc'], name, cls)  # HACK: z jakiegoś powodu cls należy do modułu `abc` :shrug:\n",
    "    \n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "distributions = dict(\n",
    "    kernel=['linear', 'rbf', 'poly'],\n",
    "    degree=[1, 2, 3],\n",
    "    C=[0.01, 0.1, 0.5, 1, 2, 5, 10, 100],\n",
    "    gamma=['auto', 100, 10, 1, 0.1, 10, 0.01, 0.001]\n",
    ")\n",
    "\n",
    "# NOTE: comment to perform full optimization\n",
    "distributions = dict(\n",
    "    kernel=['rbf'], C=[10], gamma=['auto']\n",
    ")\n",
    "\n",
    "clf = RandomizedSearchCV(\n",
    "    timeout(SVC, timeout=120)(), \n",
    "    distributions,\n",
    "    random_state=2,\n",
    "    verbose=1,\n",
    "    n_jobs=3,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_iter=60,\n",
    "    error_score=-np.inf,\n",
    "    refit=False,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(clf)\n",
    "\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize RandomSearchCV ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "df = pd.DataFrame(clf.cv_results_)\n",
    "df = df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score', 'mean_train_score', 'mean_fit_time']]\n",
    "df.sort_values(by='rank_test_score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best scores for given parameters combinations ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "listables = []\n",
    "for key, value in clf.param_distributions.items():\n",
    "    if isinstance(value, list):\n",
    "        listables.append(key)\n",
    "\n",
    "for a, b in itertools.combinations(listables, 2):\n",
    "#     print(a,b)\n",
    "    params_a = clf.param_distributions[a]\n",
    "    params_b = clf.param_distributions[b]\n",
    "    \n",
    "    results = []\n",
    "    for val_a, val_b in itertools.product(params_a, params_b):\n",
    "        crit_a = clf.cv_results_[f'param_{a}'] == val_a\n",
    "        crit_b = clf.cv_results_[f'param_{b}'] == val_b\n",
    "        try:\n",
    "            best = np.max(clf.cv_results_['mean_test_score'][crit_a & crit_b])\n",
    "        except ValueError:\n",
    "            best = np.nan \n",
    "        results.append(best)\n",
    "#         print('\\t', val_a, val_b, best)\n",
    "    results = np.array(results).reshape((len(params_a), len(params_b)))\n",
    "\n",
    "    plt.figure(figsize=(len(params_b), len(params_a)))\n",
    "    ax = sns.heatmap(results, xticklabels=params_b, yticklabels=params_a, annot=True, fmt=\".2f\", vmin=0.5, vmax=1.0, cmap='viridis')\n",
    "    ax.set_facecolor(\"black\")\n",
    "    ax.set_title(f\"{a} vs {b}\")\n",
    "    ax.set(xlabel=b, ylabel=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the best estimator ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n",
    "best_estimator = clf.estimator.set_params(**clf.best_params_)\n",
    "best_estimator.fit(X, y, _timeout=100)\n",
    "print()\n",
    "\n",
    "print('non-balanced:')\n",
    "evaluate_model(best_estimator, test_data)\n",
    "print()\n",
    "\n",
    "print('balanced:')\n",
    "idx, y_pred, y_true = evaluate_model(best_estimator, test_data, balanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 6\n",
    "\n",
    "################################################\n",
    "fig, axs = plt.subplots(1, N, figsize=(16, 14))\n",
    "fig.suptitle('y_true=0 y_pred=1')\n",
    "for ax, pos in zip(axs, np.random.choice(idx[(y_pred==1) & (y_true == 0)].reshape(-1), N)):\n",
    "    ax.imshow(test_data.X[pos-20:pos+20].T)\n",
    "    ax.set_title(f\"{pos}\")\n",
    "    ax.axvline(19,color='red')\n",
    "    ax.axvline(21,color='red')\n",
    "\n",
    "################################################\n",
    "fig, axs = plt.subplots(1, N, figsize=(16, 14))\n",
    "fig.suptitle('y_true=1 y_pred=0')\n",
    "for ax, pos in zip(axs, np.random.choice(idx[(y_pred==0) & (y_true == 1)].reshape(-1), N)):\n",
    "    ax.imshow(test_data.X[pos-20:pos+20].T)\n",
    "    ax.set_title(f\"{pos}\")\n",
    "    ax.axvline(19,color='red')\n",
    "    ax.axvline(21,color='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Selection file #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%time y_pred = best_estimator.predict(test_data.X)\n",
    "\n",
    "print(classification_report(test_data.y_binary, y_pred))\n",
    "\n",
    "print(Counter(y_pred))\n",
    "print(Counter(test_data.y_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict_annotations(dataset, y_pred, *, smoothing=0, min_length=0):\n",
    "    annotations = []\n",
    "    \n",
    "    for i, value in enumerate(y_pred):\n",
    "        if value == 1:\n",
    "            if len(annotations) and i - annotations[-1][1] - 1 <= smoothing:\n",
    "                annotations[-1][1] = i\n",
    "            else:\n",
    "                annotations.append([i, i])\n",
    "\n",
    "    annotations = [(a, b) for a, b in annotations if b - a + 1 >= min_length]\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "\n",
    "def create_selection_table(dataset, annotations):\n",
    "    return pd.DataFrame.from_records([\n",
    "        (i, 'Spectrogram 1', 1, dataset.frame_to_time(a), dataset.frame_to_time(b), dataset.meta['audio_name']) for i, (a, b) in enumerate(annotations, 1)\n",
    "    ], columns=[\n",
    "        'Selection',\n",
    "        'View',\n",
    "        'Channel',\n",
    "        'Begin Time (s)',\n",
    "        'End Time (s)',\n",
    "        'Begin Path',\n",
    "    ], index='Selection')\n",
    "    \n",
    "ann = predict_annotations(test_data, y_pred, smoothing=5, min_length=20)\n",
    "\n",
    "df = create_annotations_table(test_data, ann)\n",
    "\n",
    "df.to_csv(test_data.meta['audio_name'].replace('.wav', '.trimed.txt'), sep=\"\\t\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Check predictions with LIME #\n",
    "\n",
    "- https://pbiecek.github.io/PM_VEE/LIME.html\n",
    "- https://github.com/pbiecek/InterpretableMachineLearning2018S\n",
    "- biblioteka: https://github.com/marcotcr/lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Random Forest Classifier #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import unittest.mock\n",
    "# import sklearn\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# def _fit_and_score(estimator, *args, **kwargs):\n",
    "#     print('Faking fit')\n",
    "#     orig_fit_and_score(estimtor, *args, **kwargs)\n",
    "\n",
    "# class RandomizedSearchCVWithTimeout(RandomizedSearchCV):\n",
    "#     def __init__(self, estimator, param_distributions, n_iter=10, scoring=None,\n",
    "#                  n_jobs=None, iid='deprecated', refit=True,\n",
    "#                  cv=None, verbose=0, pre_dispatch='2*n_jobs',\n",
    "#                  random_state=None, error_score=np.nan,\n",
    "#                  return_train_score=False, timeout=None):\n",
    "#         super().__init__(estimator=estimator, param_distributions=param_distributions, \n",
    "#                          n_iter=n_iter, scoring=scoring, n_jobs=n_jobs, iid=iid, refit=refit,\n",
    "#                          cv=cv, verbose=verbose, pre_dispatch=pre_dispatch,\n",
    "#                          random_state=random_state, error_score=error_score,\n",
    "#                          return_train_score=return_train_score)\n",
    "        \n",
    "#         self._timeout = timeout\n",
    "#     def fit(self, *args, **kwargs):\n",
    "#         orig_fit_and_score = sklearn.model_selection._validation._fit_and_score\n",
    "        \n",
    "#         def timeouted_fit(estimator):\n",
    "#             orig = estimator.fit\n",
    "#             return orig\n",
    "        \n",
    "#         def _fit_and_score(estimator, *args, **kwargs):\n",
    "#             print('Faking fit')\n",
    "#             orig_fit_and_score(estimtor, *args, **kwargs)\n",
    "# #             with unittest.mock.patch(estimator.fit, timeouted_fit(estimator)):\n",
    "# #                 orig_fit_and_score(estimator, *args, **kwargs)\n",
    "        \n",
    "        \n",
    "#         with unittest.mock.patch('sklearn.model_selection._validation._fit_and_score', _fit_and_score):\n",
    "#             super().fit(*args, **kwargs)\n",
    "            \n",
    "            \n",
    "# # #     @property\n",
    "# # #     def verbose(self):\n",
    "# # #         frame = sys._getframe(1)\n",
    "# # #         print(dir(frame))\n",
    "# # #         if str(frame.f_code.co_filename).endswith('sklearn/model_selection/_search.py'):\n",
    "# # #             print('-'*20, frame.f_lineno)\n",
    "# # #             print(frame.f_code.co_filename)\n",
    "# # #             print('>>', frame.f_locals.keys())\n",
    "# # # #         print(sys._getframe(1).f_code.co_filename)\n",
    "# # # #         print('*'*100)\n",
    "# # #         return self._verbose\n",
    "# # #     @verbose.setter\n",
    "# # #     def verbose(self, value):\n",
    "# # #         self._verbose = value\n",
    "    \n",
    "# # #     def _run_search(self, evaluate_candidates):\n",
    "# # #         print('run_search')\n",
    "# # #         raise RuntimeError()\n",
    "        \n",
    "# # print(RandomizedSearchCVWithTimeout.mro())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# parameters = {\n",
    "#     'n_estimators': [100, 500, 1000],\n",
    "#     'max_depth': [20, 40, 100],\n",
    "# }\n",
    "\n",
    "# rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# clf = GridSearchCV(rf, parameters, cv=5, verbose=10)\n",
    "\n",
    "# clf.fit(X, y_binary)\n",
    "\n",
    "# # clf.cv_results_\n",
    "# print(clf.best_params_, clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clf.best_params_, clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx, y_pred, y_true = evaluate_model(rf, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jak oceniać modele? jaka miara? ##\n",
    "## Jak normalizować dane? potrzebne dla SVMa ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
