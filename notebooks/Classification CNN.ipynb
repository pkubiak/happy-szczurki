{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bcc1362f5e5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn import  metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from happy_szczurki.models.CNN import ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace \n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        dataset = dict(np.load(self.path, allow_pickle=True))\n",
    "        \n",
    "        self.X = dataset.pop('X')\n",
    "        self.y = dataset.pop('y')\n",
    "        self.meta = dataset.pop('meta')[()]\n",
    "        \n",
    "        assert len(dataset) == 0\n",
    "        \n",
    "        self.y_binary = np.where(self.y == None, 0, 1)\n",
    "    \n",
    "    def normalize(self, mean=None, std=None):\n",
    "        # TODO: czy powinniśmy normalizować per współrzędna czy globalnie?\n",
    "        if mean is None:\n",
    "            mean = np.mean(self.X, axis=0)\n",
    "            \n",
    "        if std is None:\n",
    "            std = np.std(self.X, axis=0, ddof=1)\n",
    "            \n",
    "        # NOTE: inplace operators to prevent memory allocations\n",
    "        self.X -= mean\n",
    "        self.X /= std\n",
    "        \n",
    "        return mean, std\n",
    "    \n",
    "    def sample(self, n, *, balanced=False, with_idx=False, random_state=None, x_with_frame=False) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Choice `n` random samples from dataset.\n",
    "        \n",
    "        @param n: number of random samples to choose,\n",
    "        @param balanced: if True number of samples for each class will be aprox. equal,\n",
    "        @param with_idx: return data indexes of sampled records,\n",
    "        @param random_state: random state used to generate samples indices,\n",
    "        \"\"\"\n",
    "        assert len(self.y.shape) == 1\n",
    "        \n",
    "        if balanced:\n",
    "            counts = Counter(self.y)\n",
    "            class_count = len(counts)\n",
    "            \n",
    "            # NOTE: https://stackoverflow.com/questions/35215161/most-efficient-way-to-map-function-over-numpy-array/35216364\n",
    "            probs = np.array([1.0 / (class_count * counts[x]) for x in self.y])\n",
    "        else:\n",
    "            probs = None\n",
    "            \n",
    "        idx = np.random.RandomState(random_state).choice(self.y.size, size=n, p=probs)\n",
    "        \n",
    "        new_X = np.pad(self.X, pad_width=[(128, 128), (0, 0)], mode='edge') \n",
    "        \n",
    "        if x_with_frame:\n",
    "            X = np.array([new_X[index: index + 257] for index in idx])\n",
    "        else:\n",
    "            X = new_X[idx + 128]\n",
    "        \n",
    "        if with_idx:\n",
    "            return idx, X, self.y[idx]\n",
    "        \n",
    "        return X, self.y[idx]\n",
    "    \n",
    "    def frame_to_time(self, frame: int) -> float:\n",
    "        \"\"\"Convert frame id to time in sec.\"\"\"\n",
    "        return librosa.core.frames_to_time(\n",
    "            frame,\n",
    "            sr=self.meta['sampling_rate'],\n",
    "            hop_length=self.meta['hop_length'],\n",
    "            n_fft=self.meta['n_fft']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ch1-2018-11-20_10-29-02_0000012.wav.trimed.npz Counter({0: 196372, 1: 38013})\n",
      "mean: 0.000000; std: 1.000209\n",
      "\n",
      "ch1-2018-11-20_10-31-42_0000014.wav.trimed.npz Counter({0: 146541, 1: 87844})\n",
      "mean: 0.000000; std: 1.000337\n"
     ]
    }
   ],
   "source": [
    "train_data = test_data = None\n",
    "train_data = Dataset('ch1-2018-11-20_10-29-02_0000012.wav.trimed.npz')\n",
    "print(train_data.path, Counter(train_data.y_binary))\n",
    "\n",
    "mean, std = train_data.normalize()\n",
    "print(\"mean: %f; std: %f\" % (np.mean(train_data.X), np.std(train_data.X, ddof=1)))\n",
    "\n",
    "#################################################################\n",
    "print()\n",
    "test_data = Dataset('ch1-2018-11-20_10-31-42_0000014.wav.trimed.npz')\n",
    "print(test_data.path, Counter(test_data.y_binary))\n",
    "\n",
    "mean, std = test_data.normalize()\n",
    "print(\"mean: %f; std: %f\" % (np.mean(test_data.X), np.std(test_data.X, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.place(train_data.y, train_data.y == None, \"None\")\n",
    "np.place(test_data.y, test_data.y == None, \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['CMP', 'FL', 'FM', 'IU', 'None', 'RP', 'SH', 'ST', 'TR'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc_train = OrdinalEncoder(dtype=int)\n",
    "enc_train.fit(train_data.y.reshape(-1, 1))\n",
    "enc_train.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['22kHz', 'CMP', 'FL', 'FM', 'None', 'RP', 'SH', 'ST', 'TR'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_test = OrdinalEncoder(dtype=int)\n",
    "enc_test.fit(test_data.y.reshape(-1, 1))\n",
    "enc_test.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConvNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-af239dde4776>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m ]\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConvNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# Loss and optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ConvNet' is not defined"
     ]
    }
   ],
   "source": [
    "layer_params = [\n",
    "    {\n",
    "        \"output_size\": 32,\n",
    "        \"kernel_size\": 5,\n",
    "        \"stride\": 1,\n",
    "        \"padding\": 2,\n",
    "        \"max_pool_kernel\": 3,\n",
    "        \"max_pool_stride\": 2\n",
    "    },\n",
    "    {\n",
    "        \"output_size\": 64,\n",
    "        \"kernel_size\": 5,\n",
    "        \"stride\": 1,\n",
    "        \"padding\": 2,\n",
    "        \"max_pool_kernel\": 2,\n",
    "        \"max_pool_stride\": 2\n",
    "    },\n",
    "    {\n",
    "        \"output_size\": 128,\n",
    "        \"kernel_size\": 5,\n",
    "        \"stride\": 1,\n",
    "        \"padding\": 2,\n",
    "        \"max_pool_kernel\": 5,\n",
    "        \"max_pool_stride\": 3\n",
    "    },\n",
    "    {\n",
    "        \"output_size\": 256,\n",
    "        \"kernel_size\": 5,\n",
    "        \"stride\": 1,\n",
    "        \"padding\": 2,\n",
    "        \"max_pool_kernel\": 5,\n",
    "        \"max_pool_stride\": 2\n",
    "    }\n",
    "]\n",
    "\n",
    "hidden_dims = [\n",
    "    (8 * 8 * 256), 100, 9\n",
    "]\n",
    "\n",
    "model = ConvNet(layer_params, hidden_dims)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "num_epochs = 20\n",
    "total_step = 20\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "test_acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i in range(total_step):\n",
    "        images, labels = train_data.sample(1, balanced=True, with_idx=False, x_with_frame=True)\n",
    "        images = torch.tensor(np.expand_dims(images, axis=1))\n",
    "        labels = torch.tensor(enc_train.transform(labels.reshape(-1, 1)).reshape(-1), dtype=torch.long)\n",
    "        \n",
    "        # Run the forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "#         print(loss)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(predicted)\n",
    "        print(labels)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "        \n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))\n",
    "        \n",
    "    # Test after epoch\n",
    "    \n",
    "    images, labels = test_data.sample(1, balanced=True, with_idx=False, x_with_frame=True)\n",
    "    images = torch.tensor(np.expand_dims(images, axis=1))\n",
    "    labels = torch.tensor(enc_test.transform(labels.reshape(-1, 1)).reshape(-1), dtype=torch.long)\n",
    "\n",
    "    # Run the forward pass\n",
    "    outputs = model(images)\n",
    "    \n",
    "    total = labels.size(0)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print(predicted)\n",
    "    print(labels)\n",
    "    acc = (predicted == labels).sum().item() / total\n",
    "    print(\"TEST ACC: \", acc )\n",
    "    test_acc_list.append(acc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try:\n",
    "- without equal sampling\n",
    "- different test file\n",
    "- larger train and/or test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
